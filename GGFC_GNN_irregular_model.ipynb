{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d84be12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "--- Running Seed: 16 (1/5) ---\n",
      "Graph learning finished in 46.47s.\n",
      "Starting GNN training...\n",
      "Epoch 01000/20000 | Train Loss: 0.262025 | Val Loss: 0.261253\n",
      "Epoch 02000/20000 | Train Loss: 0.241189 | Val Loss: 0.242564\n",
      "Epoch 03000/20000 | Train Loss: 0.222384 | Val Loss: 0.226037\n",
      "Epoch 04000/20000 | Train Loss: 0.194513 | Val Loss: 0.195792\n",
      "Epoch 05000/20000 | Train Loss: 0.172639 | Val Loss: 0.180263\n",
      "Epoch 06000/20000 | Train Loss: 0.150742 | Val Loss: 0.159126\n",
      "Epoch 07000/20000 | Train Loss: 0.128686 | Val Loss: 0.141708\n",
      "Epoch 08000/20000 | Train Loss: 0.104011 | Val Loss: 0.119849\n",
      "Epoch 09000/20000 | Train Loss: 0.087086 | Val Loss: 0.109496\n",
      "Epoch 10000/20000 | Train Loss: 0.078166 | Val Loss: 0.107794\n",
      "Epoch 11000/20000 | Train Loss: 0.070574 | Val Loss: 0.104746\n",
      "Epoch 12000/20000 | Train Loss: 0.067351 | Val Loss: 0.105331\n",
      "Early stopping at epoch 12544!\n",
      "\n",
      "[RESULTS FOR SEED 16]\n",
      "  Recall: 0.6198 | Precision: 0.6122 | F1: 0.6146 | Jaccard (IoU): 0.8425\n",
      "\n",
      "--- Running Seed: 24 (2/5) ---\n",
      "Graph learning finished in 44.43s.\n",
      "Starting GNN training...\n",
      "Epoch 01000/20000 | Train Loss: 0.268786 | Val Loss: 0.260886\n",
      "Epoch 02000/20000 | Train Loss: 0.237408 | Val Loss: 0.227612\n",
      "Epoch 03000/20000 | Train Loss: 0.214317 | Val Loss: 0.205919\n",
      "Epoch 04000/20000 | Train Loss: 0.182785 | Val Loss: 0.180626\n",
      "Epoch 05000/20000 | Train Loss: 0.154662 | Val Loss: 0.145431\n",
      "Epoch 06000/20000 | Train Loss: 0.135803 | Val Loss: 0.133806\n",
      "Epoch 07000/20000 | Train Loss: 0.114170 | Val Loss: 0.117147\n",
      "Epoch 08000/20000 | Train Loss: 0.099356 | Val Loss: 0.108993\n",
      "Epoch 09000/20000 | Train Loss: 0.086965 | Val Loss: 0.104767\n",
      "Epoch 10000/20000 | Train Loss: 0.080424 | Val Loss: 0.102232\n",
      "Epoch 11000/20000 | Train Loss: 0.074088 | Val Loss: 0.102251\n",
      "Epoch 12000/20000 | Train Loss: 0.064183 | Val Loss: 0.097625\n",
      "Epoch 13000/20000 | Train Loss: 0.061392 | Val Loss: 0.097666\n",
      "Epoch 14000/20000 | Train Loss: 0.059469 | Val Loss: 0.094952\n",
      "Early stopping at epoch 14684!\n",
      "\n",
      "[RESULTS FOR SEED 24]\n",
      "  Recall: 0.6142 | Precision: 0.6152 | F1: 0.6138 | Jaccard (IoU): 0.8400\n",
      "\n",
      "--- Running Seed: 45 (3/5) ---\n",
      "Graph learning finished in 44.77s.\n",
      "Starting GNN training...\n",
      "Epoch 01000/20000 | Train Loss: 0.268199 | Val Loss: 0.274367\n",
      "Epoch 02000/20000 | Train Loss: 0.248107 | Val Loss: 0.258700\n",
      "Epoch 03000/20000 | Train Loss: 0.221888 | Val Loss: 0.233655\n",
      "Epoch 04000/20000 | Train Loss: 0.207231 | Val Loss: 0.218265\n",
      "Epoch 05000/20000 | Train Loss: 0.163455 | Val Loss: 0.185464\n",
      "Epoch 06000/20000 | Train Loss: 0.143011 | Val Loss: 0.170452\n",
      "Epoch 07000/20000 | Train Loss: 0.122169 | Val Loss: 0.153521\n",
      "Epoch 08000/20000 | Train Loss: 0.099637 | Val Loss: 0.133794\n",
      "Epoch 09000/20000 | Train Loss: 0.085380 | Val Loss: 0.125317\n",
      "Epoch 10000/20000 | Train Loss: 0.075431 | Val Loss: 0.117235\n",
      "Epoch 11000/20000 | Train Loss: 0.070788 | Val Loss: 0.114485\n",
      "Epoch 12000/20000 | Train Loss: 0.064820 | Val Loss: 0.113527\n",
      "Epoch 13000/20000 | Train Loss: 0.059717 | Val Loss: 0.112852\n",
      "Epoch 14000/20000 | Train Loss: 0.056877 | Val Loss: 0.111450\n",
      "Epoch 15000/20000 | Train Loss: 0.052636 | Val Loss: 0.110932\n",
      "Early stopping at epoch 15611!\n",
      "\n",
      "[RESULTS FOR SEED 45]\n",
      "  Recall: 0.6122 | Precision: 0.6143 | F1: 0.6124 | Jaccard (IoU): 0.8432\n",
      "\n",
      "--- Running Seed: 54 (4/5) ---\n",
      "Graph learning finished in 44.54s.\n",
      "Starting GNN training...\n",
      "Epoch 01000/20000 | Train Loss: 0.259711 | Val Loss: 0.262419\n",
      "Epoch 02000/20000 | Train Loss: 0.240670 | Val Loss: 0.245911\n",
      "Epoch 03000/20000 | Train Loss: 0.221121 | Val Loss: 0.229135\n",
      "Epoch 04000/20000 | Train Loss: 0.188235 | Val Loss: 0.190726\n",
      "Epoch 05000/20000 | Train Loss: 0.160891 | Val Loss: 0.165746\n",
      "Epoch 06000/20000 | Train Loss: 0.140464 | Val Loss: 0.150453\n",
      "Epoch 07000/20000 | Train Loss: 0.123350 | Val Loss: 0.140492\n",
      "Epoch 08000/20000 | Train Loss: 0.108220 | Val Loss: 0.130052\n",
      "Epoch 09000/20000 | Train Loss: 0.094805 | Val Loss: 0.120041\n",
      "Epoch 10000/20000 | Train Loss: 0.086133 | Val Loss: 0.116186\n",
      "Epoch 11000/20000 | Train Loss: 0.078670 | Val Loss: 0.113768\n",
      "Epoch 12000/20000 | Train Loss: 0.070076 | Val Loss: 0.110011\n",
      "Epoch 13000/20000 | Train Loss: 0.064966 | Val Loss: 0.109162\n",
      "Epoch 14000/20000 | Train Loss: 0.061434 | Val Loss: 0.107668\n",
      "Epoch 15000/20000 | Train Loss: 0.056839 | Val Loss: 0.105316\n",
      "Epoch 16000/20000 | Train Loss: 0.052810 | Val Loss: 0.103625\n",
      "Early stopping at epoch 16138!\n",
      "\n",
      "[RESULTS FOR SEED 54]\n",
      "  Recall: 0.6153 | Precision: 0.6165 | F1: 0.6152 | Jaccard (IoU): 0.8375\n",
      "\n",
      "--- Running Seed: 65 (5/5) ---\n",
      "Graph learning finished in 42.36s.\n",
      "Starting GNN training...\n",
      "Epoch 01000/20000 | Train Loss: 0.276494 | Val Loss: 0.275285\n",
      "Epoch 02000/20000 | Train Loss: 0.243045 | Val Loss: 0.250875\n",
      "Epoch 03000/20000 | Train Loss: 0.227773 | Val Loss: 0.235186\n",
      "Epoch 04000/20000 | Train Loss: 0.189518 | Val Loss: 0.194944\n",
      "Epoch 05000/20000 | Train Loss: 0.158336 | Val Loss: 0.163302\n",
      "Epoch 06000/20000 | Train Loss: 0.136676 | Val Loss: 0.147434\n",
      "Epoch 07000/20000 | Train Loss: 0.115346 | Val Loss: 0.129353\n",
      "Epoch 08000/20000 | Train Loss: 0.096902 | Val Loss: 0.115352\n",
      "Epoch 09000/20000 | Train Loss: 0.086116 | Val Loss: 0.107766\n",
      "Epoch 10000/20000 | Train Loss: 0.077885 | Val Loss: 0.104150\n",
      "Epoch 11000/20000 | Train Loss: 0.072274 | Val Loss: 0.099971\n",
      "Epoch 12000/20000 | Train Loss: 0.067437 | Val Loss: 0.099030\n",
      "Epoch 13000/20000 | Train Loss: 0.063130 | Val Loss: 0.096195\n",
      "Epoch 14000/20000 | Train Loss: 0.057967 | Val Loss: 0.095774\n",
      "Epoch 15000/20000 | Train Loss: 0.055371 | Val Loss: 0.094918\n",
      "Early stopping at epoch 15881!\n",
      "\n",
      "[RESULTS FOR SEED 65]\n",
      "  Recall: 0.6170 | Precision: 0.6143 | F1: 0.6150 | Jaccard (IoU): 0.8433\n",
      "\n",
      "\n",
      "#############################################\n",
      "###      EXPERIMENT FINAL SUMMARY         ###\n",
      "#############################################\n",
      "\n",
      "--- Aggregated Results (over 5 seeds) ---\n",
      "\n",
      "Jaccard (IoU) (Micro):  Mean = 0.8413, Std = 0.0022\n",
      "Recall (Macro):         Mean = 0.6157, Std = 0.0026\n",
      "Precision (Macro):      Mean = 0.6145, Std = 0.0014\n",
      "F1-Score (Macro):       Mean = 0.6142, Std = 0.0010\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# ------------------------------------\n",
    "# Configuration & Hyperparameters\n",
    "# ------------------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- Seeds ---\n",
    "SEEDS = [16, 24, 45, 54, 65]\n",
    "GRAPH_SEED_BASE = 11\n",
    "\n",
    "# --- Graph Learning Parameters ---\n",
    "GRAPH_LEARN_EPOCHS = 5000\n",
    "GRAPH_LEARN_ALPHA = 1.0\n",
    "GRAPH_LEARN_BETA = 2.0\n",
    "GRAPH_LEARN_ETA = 0.001\n",
    "\n",
    "# --- GNN Model & Training Parameters ---\n",
    "SAGE_DROPOUT = 0.2\n",
    "LR = 4e-4\n",
    "EPOCHS = 20000\n",
    "PATIENCE = 1000\n",
    "\n",
    "# --- HYPERPARAMETER ---\n",
    "K_VAL = 0.00001\n",
    "\n",
    "# ------------------------------------\n",
    "# Dice Loss for Binary Classification\n",
    "# ------------------------------------\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Dice loss, suitable for binary segmentation or multi-label classification tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice_coeff = (2. * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)\n",
    "        return 1 - dice_coeff\n",
    "\n",
    "# ------------------------------------\n",
    "# Learn Graph Structure (GPU Accelerated)\n",
    "# ------------------------------------\n",
    "def learn_graph_gpu(X: torch.Tensor, seed_graph: int, pruning_k_percent: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Learns a graph structure from node features entirely on the GPU.\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    # Set the seed for graph learning to make it reproducible for a given seed\n",
    "    torch.manual_seed(seed_graph)\n",
    "    i_idx, j_idx = torch.triu_indices(N, N, offset=1, device=DEVICE)\n",
    "    w = torch.randn(len(i_idx), device=DEVICE) * 0.01 + 0.5\n",
    "    w.requires_grad = True\n",
    "    delL = torch.sum((X[i_idx] - X[j_idx]) ** 2, dim=1)\n",
    "    optimizer = torch.optim.Adam([w], lr=GRAPH_LEARN_ETA)\n",
    "\n",
    "    for epoch in range(GRAPH_LEARN_EPOCHS):\n",
    "        optimizer.zero_grad()\n",
    "        w_clipped = torch.clamp(w, min=0)\n",
    "        degrees = torch.zeros(N, device=DEVICE)\n",
    "        degrees.index_add_(0, i_idx, w_clipped)\n",
    "        degrees.index_add_(0, j_idx, w_clipped)\n",
    "        \n",
    "        loss_laplacian = torch.sum(delL * w_clipped)\n",
    "        loss_beta = (GRAPH_LEARN_BETA / 2.0) * torch.sum(w_clipped ** 2)\n",
    "        loss_alpha = -GRAPH_LEARN_ALPHA * torch.sum(torch.log(degrees + 1e-12))\n",
    "        loss = loss_laplacian + loss_beta + loss_alpha\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    best_w = torch.clamp(w.detach(), min=0)\n",
    "    num_edges = len(best_w)\n",
    "    k = max(1, int(pruning_k_percent * num_edges))\n",
    "    \n",
    "    if k < num_edges:\n",
    "        threshold = torch.kthvalue(best_w, num_edges - k).values\n",
    "        top_mask = best_w >= threshold\n",
    "    else:\n",
    "        top_mask = torch.ones_like(best_w, dtype=torch.bool)\n",
    "\n",
    "    final_i = i_idx[top_mask]\n",
    "    final_j = j_idx[top_mask]\n",
    "    edge_index = torch.stack([torch.cat([final_i, final_j]), torch.cat([final_j, final_i])], dim=0)\n",
    "    return edge_index\n",
    "\n",
    "# ------------------------------------\n",
    "# GNN Model\n",
    "# ------------------------------------\n",
    "class GraphSAGENet(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, 256)\n",
    "        self.conv2 = SAGEConv(256, 512)\n",
    "        self.conv3 = SAGEConv(512, 512)\n",
    "        self.conv4 = SAGEConv(512, 256)\n",
    "        self.out = torch.nn.Linear(256, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=SAGE_DROPOUT, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.dropout(x, p=SAGE_DROPOUT, training=self.training)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.dropout(x, p=SAGE_DROPOUT, training=self.training)\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.dropout(x, p=SAGE_DROPOUT, training=self.training)\n",
    "        return self.out(x)\n",
    "\n",
    "# ------------------------------------\n",
    "# Train and Evaluate for One Seed\n",
    "# ------------------------------------\n",
    "def run_single_seed(X_raw, Y_raw, seed_model, seed_graph, pruning_k_percent):\n",
    "    \"\"\"\n",
    "    Trains and evaluates the model for a single random seed and a given k.\n",
    "    \"\"\"\n",
    "    # Set seeds for reproducibility for this specific run\n",
    "    torch.manual_seed(seed_model)\n",
    "    np.random.seed(seed_model)\n",
    "    if DEVICE.type == 'cuda':\n",
    "        torch.cuda.manual_seed_all(seed_model)\n",
    "\n",
    "    # Preprocessing\n",
    "    scaler = StandardScaler()\n",
    "    X_np_abs = np.abs(X_raw.numpy())\n",
    "    X_scaled_np = scaler.fit_transform(X_np_abs)\n",
    "    X = torch.tensor(X_scaled_np, dtype=torch.float32).to(DEVICE)\n",
    "    Y = Y_raw.to(DEVICE)\n",
    "\n",
    "    # Graph Learning\n",
    "    start_time = time.time()\n",
    "    edge_index = learn_graph_gpu(X, seed_graph, pruning_k_percent=pruning_k_percent)\n",
    "    print(f\"Graph learning finished in {time.time() - start_time:.2f}s.\")\n",
    "\n",
    "    data = Data(x=X, y=Y, edge_index=edge_index)\n",
    "\n",
    "    # --- MODIFIED: 80/10/10 Random Data Split ---\n",
    "    num_samples = X.shape[0]\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices) # Shuffle indices randomly\n",
    "\n",
    "    train_size = int(0.8 * num_samples)\n",
    "    val_size = int(0.1 * num_samples)\n",
    "    \n",
    "    train_idx = indices[:train_size]\n",
    "    val_idx = indices[train_size : train_size + val_size]\n",
    "    test_idx = indices[train_size + val_size :]\n",
    "    # --- END MODIFICATION ---\n",
    "\n",
    "    data.train_mask = torch.zeros(X.shape[0], dtype=torch.bool, device=DEVICE)\n",
    "    data.train_mask[train_idx] = True\n",
    "    data.val_mask = torch.zeros(X.shape[0], dtype=torch.bool, device=DEVICE)\n",
    "    data.val_mask[val_idx] = True\n",
    "    data.test_mask = torch.zeros(X.shape[0], dtype=torch.bool, device=DEVICE)\n",
    "    data.test_mask[test_idx] = True\n",
    "\n",
    "    # Model Initialization and Training\n",
    "    model = GraphSAGENet(X.size(1), Y.size(1)).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    criterion = DiceLoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    print(\"Starting GNN training...\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(data.x, data.edge_index)\n",
    "            val_loss = criterion(val_out[data.val_mask], data.y[data.val_mask])\n",
    "        \n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print(f\"Epoch {epoch+1:05d}/{EPOCHS} | Train Loss: {loss.item():.6f} | Val Loss: {val_loss.item():.6f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch+1}!\")\n",
    "            break\n",
    "\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    else:\n",
    "        print(\"Warning: Early stopping did not trigger. Using the last model state.\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "\n",
    "    test_logits = out[data.test_mask]\n",
    "    test_true = data.y[data.test_mask]\n",
    "    test_preds = (torch.sigmoid(test_logits) > 0.5).int()\n",
    "\n",
    "    test_preds_np = test_preds.cpu().numpy()\n",
    "    test_true_np = test_true.cpu().numpy()\n",
    "\n",
    "    # ##################################################################\n",
    "    # ########### START: MODIFIED EVALUATION BLOCK #####################\n",
    "    # ##################################################################\n",
    "\n",
    "    # --- JACCARD INDEX (IoU) CALCULATION (MICRO-AVERAGED) ---\n",
    "    intersection = np.sum(test_true_np * test_preds_np)\n",
    "    union = np.sum(np.logical_or(test_true_np, test_preds_np).astype(int))\n",
    "    micro_jaccard = 1.0 if union == 0 else intersection / union\n",
    "\n",
    "    # --- OTHER METRICS (MACRO-AVERAGED) ---\n",
    "    # These metrics calculate the score for each label independently and then average them.\n",
    "    recall = recall_score(test_true_np, test_preds_np, average='macro', zero_division=0)\n",
    "    precision = precision_score(test_true_np, test_preds_np, average='macro', zero_division=0)\n",
    "    f1 = f1_score(test_true_np, test_preds_np, average='macro', zero_division=0)\n",
    "\n",
    "    return recall, f1, precision, micro_jaccard\n",
    "\n",
    "    # ##################################################################\n",
    "    # ############ END: MODIFIED EVALUATION BLOCK ######################\n",
    "    # ##################################################################\n",
    "\n",
    "\n",
    "# ------------------------------------\n",
    "# Main Execution\n",
    "# ------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        df = pd.read_csv(\"2500_gravity_anomaly_irregular_shape.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: '2500_gravity_anomaly_irregular_shape.csv' not found.\")\n",
    "        print(\"Please make sure the CSV file is in the same directory as the script.\")\n",
    "        exit()\n",
    "\n",
    "    required_samples = 2500\n",
    "    if df.shape[0] < required_samples:\n",
    "        print(f\"Error: Dataset has only {df.shape[0]} samples, but at least {required_samples} are recommended.\")\n",
    "        exit()\n",
    "\n",
    "    X_raw = torch.tensor(df.iloc[:, :22].values, dtype=torch.float32)\n",
    "    Y_raw = torch.tensor(df.iloc[:, 22:274].values, dtype=torch.float32)\n",
    "\n",
    "    all_results = {'rec': [], 'f1': [], 'prec': [], 'jaccard': []}\n",
    "\n",
    "    for i, seed in enumerate(SEEDS):\n",
    "        print(f\"\\n--- Running Seed: {seed} ({i+1}/{len(SEEDS)}) ---\")\n",
    "        \n",
    "        # --- MODIFIED: Pass the changing seed to the graph learning function ---\n",
    "        rec, f1, prec, jaccard = run_single_seed(\n",
    "            X_raw, Y_raw,\n",
    "            seed_model=seed,\n",
    "            seed_graph=seed, # Use the main loop seed for graph randomization\n",
    "            pruning_k_percent=K_VAL\n",
    "        )\n",
    "        # --- END MODIFICATION ---\n",
    "        \n",
    "        print(f\"\\n[RESULTS FOR SEED {seed}]\")\n",
    "        print(f\"  Recall: {rec:.4f} | Precision: {prec:.4f} | F1: {f1:.4f} | Jaccard (IoU): {jaccard:.4f}\")\n",
    "\n",
    "        all_results['rec'].append(rec)\n",
    "        all_results['f1'].append(f1)\n",
    "        all_results['prec'].append(prec)\n",
    "        all_results['jaccard'].append(jaccard)\n",
    "\n",
    "    print(\"\\n\\n#############################################\")\n",
    "    print(\"###      EXPERIMENT FINAL SUMMARY         ###\")\n",
    "    print(\"#############################################\")\n",
    "    print(f\"\\n--- Aggregated Results (over {len(SEEDS)} seeds) ---\\n\")\n",
    "\n",
    "    recall = np.array(all_results['rec'])\n",
    "    precision = np.array(all_results['prec'])\n",
    "    f1 = np.array(all_results['f1'])\n",
    "    jaccard = np.array(all_results['jaccard'])\n",
    "\n",
    "    print(f\"Jaccard (IoU) (Micro):  Mean = {jaccard.mean():.4f}, Std = {jaccard.std():.4f}\")\n",
    "    print(f\"Recall (Macro):         Mean = {recall.mean():.4f}, Std = {recall.std():.4f}\")\n",
    "    print(f\"Precision (Macro):      Mean = {precision.mean():.4f}, Std = {precision.std():.4f}\")\n",
    "    print(f\"F1-Score (Macro):       Mean = {f1.mean():.4f}, Std = {f1.std():.4f}\")\n",
    "    print(\"---------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimesh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
